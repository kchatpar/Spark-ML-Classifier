{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis on Titanic Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF takes 3 s\n",
      "Area under PR = 0.5752939929838485\n",
      "Area under ROC = 0.7584256138661533\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|  22|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|  38|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|  26|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|  35|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|  35|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|  54|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male|   2|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|  27|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|  14|    1|    0|          237736|30.0708| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------+--------+--------------------+\n",
      "|prediction|Survived|            features|\n",
      "+----------+--------+--------------------+\n",
      "|       1.0|       0|[3.0,0.0,9.0,1.0,...|\n",
      "|       0.0|       0|(7,[0,2,4],[3.0,3...|\n",
      "|       1.0|       0|[2.0,0.0,24.0,0.0...|\n",
      "|       1.0|       0|[2.0,0.0,38.0,0.0...|\n",
      "|       1.0|       0|[2.0,0.0,44.0,1.0...|\n",
      "+----------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.159091\n",
      "RandomForestClassificationModel (uid=RandomForestClassifier_46da8038d4d5796d4461) with 20 trees\n",
      "Accuracy = 0.840909\n",
      "f1 = 0.835959\n",
      "weightedPrecision = 0.845324\n",
      "weightedRecall = 0.840909\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pyspark\n",
    "import os\n",
    "import csv\n",
    "from numpy import array\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, OneHotEncoder, VectorAssembler, IndexToString\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "# sc = pyspark.SparkContext(conf=conf) //RUN THIS IF YOUR SPARK CONTEXT IS NOT CREATED\n",
    "conf.getAll()\n",
    "\n",
    "# Reading from the hdfs, removing the header\n",
    "trainTitanic= sc.textFile(\"train.csv\")\n",
    "trainHeader = trainTitanic.first()\n",
    "trainTitanic = trainTitanic.filter(lambda line: line != trainHeader).mapPartitions(lambda x: csv.reader(x))\n",
    "trainTitanic.first()\n",
    " \n",
    "# Data preprocessing\n",
    "def sexTransformMapper(elem):\n",
    "    '''Function which transform \"male\" into 1 and else things into 0\n",
    "    - elem : string\n",
    "    - return : vector\n",
    "    '''\n",
    "     \n",
    "    if elem == 'male' :\n",
    "        return [0]\n",
    "    else :\n",
    "        return [1]\n",
    " \n",
    "# Data Transformations and filter lines with empty strings\n",
    "trainTitanic=trainTitanic.map(lambda line: line[1:3]+sexTransformMapper(line[4])+line[5:11])\n",
    "trainTitanic=trainTitanic.filter(lambda line: line[3] != '' ).filter(lambda line: line[4] != '' )\n",
    "trainTitanic.take(10)\n",
    " \n",
    "# creating \"labeled point\" rdds specific to MLlib \"(label (v1, v2...vp])\"\n",
    "trainTitanicLP=trainTitanic.map(lambda line: LabeledPoint(line[0],[line[1:5]]))\n",
    "trainTitanicLP.first()\n",
    " \n",
    "# splitting dataset into train and test set\n",
    "(trainData, testData) = trainTitanicLP.randomSplit([0.7, 0.3])\n",
    " \n",
    "# Random forest : same parameters as sklearn (?)\n",
    "from pyspark.mllib.tree import RandomForest\n",
    " \n",
    "time_start=time.time()\n",
    "model_rf = RandomForest.trainClassifier(trainData, numClasses = 2,\n",
    "        categoricalFeaturesInfo = {}, numTrees = 100,\n",
    "        featureSubsetStrategy='auto', impurity='gini', maxDepth=12,\n",
    "        maxBins=32, seed=None)\n",
    " \n",
    "  \n",
    "model_rf.numTrees()\n",
    "model_rf.totalNumNodes()\n",
    "time_end=time.time()\n",
    "time_rf=(time_end - time_start)\n",
    "print(\"RF takes %d s\" %(time_rf))\n",
    " \n",
    "# Predictions on test set\n",
    "predictions = model_rf.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    " \n",
    "# first metrics\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "metrics = BinaryClassificationMetrics(labelsAndPredictions)\n",
    " \n",
    "# Area under precision-recall curve\n",
    "print(\"Area under PR = %s\" % metrics.areaUnderPR)\n",
    " \n",
    "# Area under ROC curve\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)\n",
    "\n",
    "\n",
    "# Creatingt Spark SQL environment\n",
    "from pyspark.sql import SparkSession, HiveContext\n",
    "SparkContext.setSystemProperty(\"hive.metastore.uris\", \"thrift://nn1:9083\")\n",
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    " \n",
    "# spark is an existing SparkSession\n",
    "train = spark.read.csv(\"train.csv\", header = True)\n",
    "# Displays the content of the DataFrame to stdout\n",
    "train.show(10)\n",
    " \n",
    "# String to float on some columns of the dataset : creates a new dataset\n",
    "train = train.select(col(\"Survived\"),col(\"Sex\"),col(\"Embarked\"),col(\"Pclass\").cast(\"float\"),col(\"Age\").cast(\"float\"),col(\"SibSp\").cast(\"float\"),col(\"Fare\").cast(\"float\"))\n",
    " \n",
    "# dropping null values\n",
    "train = train.dropna()\n",
    " \n",
    "# Spliting in train and test set. Beware : It sorts the dataset\n",
    "(traindf, testdf) = train.randomSplit([0.7,0.3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "genderIndexer = StringIndexer(inputCol=\"Sex\", outputCol=\"indexedSex\")\n",
    "embarkIndexer = StringIndexer(inputCol=\"Embarked\", outputCol=\"indexedEmbarked\")\n",
    " \n",
    "surviveIndexer = StringIndexer(inputCol=\"Survived\", outputCol=\"indexedSurvived\")\n",
    " \n",
    "# One Hot Encoder on indexed features\n",
    "genderEncoder = OneHotEncoder(inputCol=\"indexedSex\", outputCol=\"sexVec\")\n",
    "embarkEncoder = OneHotEncoder(inputCol=\"indexedEmbarked\", outputCol=\"embarkedVec\")\n",
    " \n",
    "# Create the vector structured data (label,features(vector))\n",
    "assembler = VectorAssembler(inputCols=[\"Pclass\",\"sexVec\",\"Age\",\"SibSp\",\"Fare\",\"embarkedVec\"],outputCol=\"features\")\n",
    " \n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedSurvived\", featuresCol=\"features\")\n",
    " \n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[surviveIndexer, genderIndexer, embarkIndexer, genderEncoder,embarkEncoder, assembler, rf]) # genderIndexer,embarkIndexer,genderEncoder,embarkEncoder,\n",
    " \n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(traindf)\n",
    " \n",
    "# Predictions\n",
    "predictions = model.transform(testdf)\n",
    " \n",
    "# Select example rows to display.\n",
    "predictions.columns \n",
    " \n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"Survived\", \"features\").show(5)\n",
    " \n",
    "# Select (prediction, true label) and compute test error\n",
    "predictions = predictions.select(col(\"Survived\").cast(\"Float\"),col(\"prediction\"))\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    " \n",
    "rfModel = model.stages[6]\n",
    "print(rfModel)  # summary only\n",
    " \n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % accuracy)\n",
    " \n",
    "evaluatorf1 = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluatorf1.evaluate(predictions)\n",
    "print(\"f1 = %g\" % f1)\n",
    " \n",
    "evaluatorwp = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "wp = evaluatorwp.evaluate(predictions)\n",
    "print(\"weightedPrecision = %g\" % wp)\n",
    " \n",
    "evaluatorwr = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "wr = evaluatorwr.evaluate(predictions)\n",
    "print(\"weightedRecall = %g\" % wr)\n",
    " \n",
    "# close sparkcontext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Flow Explanation: We collected our articles from NYT using the scipt at the bottom. Our categories are Politics, Food, Business, and Sports. We used the method wholeTextFiles to import our articles into Spark. Once our data for all four categories are in Spark we call our main function which returns the data as a string for each category. We then create a dataframe for each category and appended the category. We then merge the dataframes creating a master dataframe. In the master dataframe, we remove the regx, remove the stopwords, calculate the TF IDF. The output is the feature vector. This gets submitted to the selected ML models and we print our accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements for code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from operator import add\n",
    "from operator import add\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml import *\n",
    "from pyspark.ml.evaluation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original remove Regx and Stop word remover - These methods are not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constants\n",
    "APP_NAME = \"Lab 3\"\n",
    "##OTHER FUNCTIONS/CLASSES\n",
    "def func(iterator):\n",
    "    global_list=[]\n",
    "    with open(\"stopWords_2.txt\", \"r\") as ins:\n",
    "        stopWords = []\n",
    "        for line in ins:\n",
    "            line=re.sub('[^A-Za-z0-9]+', '', line)\n",
    "            line=line.lower()\n",
    "            stopWords.append(line)\n",
    "    print(stopWords)\n",
    "    for x in iterator:\n",
    "        list=x.split()\n",
    "        for word in list:\n",
    "            word = re.sub('[^A-Za-z0-9]+', '', word)\n",
    "            if word.lower() not in stopWords:\n",
    "                global_list.append((word,1))\n",
    "    return global_list\n",
    "\n",
    "def regex(iterator):\n",
    "   # global_list=[]\n",
    "    for x,y in iterator:\n",
    "        y=re.sub(r'[^\\x00-\\x7F]+',' ', y)\n",
    "    return iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main method returns a list of words for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(sc,data):    \n",
    "   dataMerge = data.reduceByKey(lambda x,y:x+y)\n",
    "   dataList = dataMerge.values().collect()\n",
    "   return dataList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method for removing stop words.\n",
    "## Method for calculating word count.\n",
    "## Spark Confoguration declerations, import data from file destinations, and call the main method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_text):\n",
    "    #extract words from rdd.\n",
    "    dirtyWords = dataAdd.collect\n",
    "\n",
    "    # keep only words\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", dirtyWords)\n",
    "\n",
    "    # convert to lower case and split \n",
    "    words = letters_only_text.lower().split()\n",
    "\n",
    "    # remove stopwords\n",
    "    stopword_set = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [w for w in words if w not in stopword_set]\n",
    "\n",
    "    # join the cleaned words in a list\n",
    "    cleaned_word_list = \" \".join(meaningful_words)\n",
    "\n",
    "    return cleaned_word_list\n",
    "\n",
    "def wordCount(data):\n",
    "    #words = textRDD.flatMap(lambda x: x.split(' ')).map(lambda x: (x, 1))\n",
    "    #wordcount = data.reduceByKey(add).collect()\n",
    "    for wc in data:\n",
    "      print('aman1\\n',wordcount)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "   # Configure Spark\n",
    "   conf = SparkConf().setAppName(APP_NAME)\n",
    "   conf = conf.setMaster(\"local[*]\")\n",
    "#    sc   = SparkContext(conf=conf)\n",
    "   # Execute Main functionality\n",
    "   databond = sc.wholeTextFiles(\"finaldata/datafinal1/business/\")\n",
    "   dataobama = sc.wholeTextFiles(\"finaldata/datafinal1/food/\")\n",
    "   datatravel = sc.wholeTextFiles(\"finaldata/datafinal1/politics/\")\n",
    "   datayankees = sc.wholeTextFiles(\"finaldata/datafinal1/sports/\")\n",
    "\n",
    "   aman=main(sc,databond)\n",
    "   aman1=main(sc,dataobama)\n",
    "   aman2=main(sc,datatravel)\n",
    "   aman3=main(sc,datayankees)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframes for each category and append the category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame(aman,StringType()).na.drop()\n",
    "df=df.withColumn(\"category\",lit(('business')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=spark.createDataFrame(aman1,StringType()).na.drop()\n",
    "df1=df1.withColumn(\"category\",lit(('food')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=spark.createDataFrame(aman2,StringType()).na.drop()\n",
    "df2=df2.withColumn(\"category\",lit(('politics')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=spark.createDataFrame(aman3,StringType()).na.drop()\n",
    "df3=df3.withColumn(\"category\",lit(('sports')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce  # For Python 3.x\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "df4 =unionAll(df,df1,df2,df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the regx, stopwords, and add category labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "regToken = RegexTokenizer(inputCol=\"value\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "stopwords = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "labels = StringIndexer(inputCol = \"category\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the TF IDF assign it to rawFeatures. Create the pipeline and fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|               value|category|               words|            filtered|         rawFeatures|            features|label|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|AdvertisementAdve...|business|[advertisementadv...|[advertisementadv...|(5000,[7,26,29,34...|(5000,[7,26,29,34...|  0.0|\n",
      "|AdvertisementAdve...|business|[advertisementadv...|[advertisementadv...|(5000,[20,24,32,5...|(5000,[20,24,32,5...|  0.0|\n",
      "|AdvertisementAdve...|business|[advertisementadv...|[advertisementadv...|(5000,[15,21,24,9...|(5000,[15,21,24,9...|  0.0|\n",
      "|AdvertisementAdve...|business|[advertisementadv...|[advertisementadv...|(5000,[0,87,125,1...|(5000,[0,87,125,1...|  0.0|\n",
      "|AdvertisementAdve...|business|[advertisementadv...|[advertisementadv...|(5000,[1,10,69,70...|(5000,[1,10,69,70...|  0.0|\n",
      "|AdvertisementAdve...|business|[advertisementadv...|[advertisementadv...|(5000,[19,20,104,...|(5000,[19,20,104,...|  0.0|\n",
      "|AdvertisementAdve...|business|[advertisementadv...|[advertisementadv...|(5000,[146,247,26...|(5000,[146,247,26...|  0.0|\n",
      "|AdvertisementAdve...|business|[advertisementadv...|[advertisementadv...|(5000,[19,24,26,4...|(5000,[19,24,26,4...|  0.0|\n",
      "|AdvertisementAdve...|business|[advertisementadv...|[advertisementadv...|(5000,[1,3,24,44,...|(5000,[1,3,24,44,...|  0.0|\n",
      "|AdvertisementAdve...|business|[advertisementadv...|[advertisementadv...|(5000,[67,84,85,8...|(5000,[67,84,85,8...|  0.0|\n",
      "|AdvertisementAdve...|business|[advertisementadv...|[advertisementadv...|(5000,[7,15,26,34...|(5000,[7,15,26,34...|  0.0|\n",
      "|AdvertisementAdve...|business|[advertisementadv...|[advertisementadv...|(5000,[1,8,18,26,...|(5000,[1,8,18,26,...|  0.0|\n",
      "|AdvertisementAdve...|business|[advertisementadv...|[advertisementadv...|(5000,[11,19,24,2...|(5000,[11,19,24,2...|  0.0|\n",
      "|AdvertisementAdve...|business|[advertisementadv...|[advertisementadv...|(5000,[1,70,73,12...|(5000,[1,70,73,12...|  0.0|\n",
      "|AdvertisementAdve...|business|[advertisementadv...|[advertisementadv...|(5000,[18,70,104,...|(5000,[18,70,104,...|  0.0|\n",
      "|AdvertisementAdve...|business|[advertisementadv...|[advertisementadv...|(5000,[1,8,15,75,...|(5000,[1,8,15,75,...|  0.0|\n",
      "|AdvertisementAdve...|business|[advertisementadv...|[advertisementadv...|(5000,[1,18,76,12...|(5000,[1,18,76,12...|  0.0|\n",
      "|AdvertisementAdve...|business|[advertisementadv...|[advertisementadv...|(5000,[15,45,81,1...|(5000,[15,45,81,1...|  0.0|\n",
      "|AdvertisementAdve...|business|[advertisementadv...|[advertisementadv...|(5000,[24,32,70,1...|(5000,[24,32,70,1...|  0.0|\n",
      "|AdvertisementAdve...|business|[advertisementadv...|[advertisementadv...|(5000,[24,26,76,8...|(5000,[24,26,76,8...|  0.0|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=5000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "pipeline = Pipeline(stages=[regToken, stopwords, hashingTF, idf, labels])\n",
    "pipelineFit = pipeline.fit(df4)\n",
    "dataset = pipelineFit.transform(df4)\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data between train and test set. Run the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 294\n",
      "Test Dataset Count: 106\n",
      "validation Dataset Count: 74\n"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = dataset.randomSplit([0.75, 0.25], seed = 10)\n",
    "(trainingDatafinal, validationData) = trainingData.randomSplit([0.7, 0.23], seed = 10)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))\n",
    "print(\"validation Dataset Count: \" + str(validationData.count()))\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions = lrModel.transform(testData)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is :73.71977105960539\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy is :\"+str(evaluator.evaluate(predictions)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression on train and validation data for the testing of the random articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingDatafinal)\n",
    "predictions = lrModel.transform(validationData)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is :71.38129921987651\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy is :\"+str(evaluator.evaluate(predictions)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest on test and train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 50, \\\n",
    "                            maxDepth = 4, \\\n",
    "                            maxBins = 32)\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData)\n",
    "predictions = rfModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is :78.50253060509209\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"The accuracy is :\"+str(evaluator.evaluate(predictions)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New York Times Scraping Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from newspaper import Article\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "MY_API_KEY = '2a58a1936b8c465c994b5f77c73a68b8'\n",
    "\n",
    "save_path = 'C:/Users/aman/Desktop/datafinal/sports/'\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "def get_articles(api_key,date,category):\n",
    "    url = 'http://api.nytimes.com/svc/search/v2/articlesearch.json'\n",
    "    url += '?q=%s&begin_date=%s&api-key=%s&page=9' % (category,date,api_key)\n",
    "    #print(url)\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    #data = json.loads(response.text)\n",
    "    articles = data['response']['docs']\n",
    "    i=81\n",
    "    for article1 in articles:\n",
    "        test = \"testfile\"\n",
    "        test +='%s'%(i)\n",
    "        i=i+1\n",
    "        print(article1['web_url'])\n",
    "        req = session.get(article1['web_url'])\n",
    "        soup = BeautifulSoup(req.text,'lxml')\n",
    "        paragraphs = soup.find_all('p')\n",
    "        article = paragraphs[0].get_text()\n",
    "#         article12=Article(article1['web_url'])\n",
    "#         article12.download()\n",
    "#         article12.parse()\n",
    "#         article12.nlp()\n",
    "        for p in paragraphs:\n",
    "            article = article + p.get_text()\n",
    "#         a=article12.text\n",
    "        name_of_file = test\n",
    "        completeName = os.path.join(save_path, name_of_file+\".txt\")         \n",
    "        file1 = open(completeName, \"w\",encoding=\"utf-8\")\n",
    "        file1.write(article)\n",
    "        file1.close()\n",
    "\n",
    "get_articles(MY_API_KEY,\"20161001\",\"sports\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
